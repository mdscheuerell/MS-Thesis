summary(sourceLM.SAM)$coeff[1,2] #this is standard error of the y-intercept
#mean of each year subtract this from each data point
data$annual_mean_CO2_ML <- annual_data$mean.CO2.ML[match(data$Year, annual_data$Year)]
View(data)
data$annual_mean_d13C_ML <- annual_data$mean.d13C.ML[match(data$Year, annual_data$Year)]
data$annual_mean_CO2_SAM <- annual_data$mean.CO2.SAM[match(data$Year, annual_data$Year)]
data$annual_mean_d13C_SAM <- annual_data$mean.d13C.SAM[match(data$Year, annual_data$Year)]
ML_CO2 <- data$CO2_ML - data$annual_mean_CO2_ML
ML_CO2
ML_d13C <- data$d13C_ML - data$annual_mean_d13C_ML
ML_d13C
#take the original data point and subtract the mean
#creates vectors of data that are detrended
detrend_ML <- data$CO2_ML - data$annual_mean_CO2_ML
detrend_SAM <- data$CO2_SAM - data$annual_mean_CO2_SAM
detrend_d13C_ML <- data$d13C_ML - data$annual_mean_d13C_ML
detrend_d13C_SAM <- data$d13C_ML - data$annual_mean_d13C_SAM
#make a new data frame with the detrended data
detrend <- data.frame(detrend_d13C_ML, detrend_ML, detrend_d13C_SAM,detrend_SAM, data$Year)
#plot
ggplot(detrend, aes(x = data.Year, y = detrend_d13C_ML)) +
geom_point(color = "red") +
geom_point(aes(data.Year, detrend_ML), color = "blue")
ggplot(detrend, aes(x = data.Year, y = detrend_d13C_SAM)) +
geom_point(color = "red") +
geom_point(aes(data.Year, detrend_SAM), color = "blue")
View(detrend)
#plot for ML
ggplot(detrend, aes(x = data.Year, y = detrend_d13C_ML)) +
geom_point(color = "red") +
geom_point(aes(data.Year, detrend_ML))
x.lm.ML <- 1/detrend$detrend_ML
y.lm.ML <- detrend$detrend_d13C_ML
ggplot(detrend, aes(x = data.Year, y = detrend_d13C_SAM)) +
geom_point(color = "red") +
geom_point(aes(data.Year, detrend_SAM), color = "blue")
x.lm.ML <- 1/detrend$detrend_ML
y.lm.ML <- detrend$detrend_d13C_ML
LM.ML <- lm(y.lm.ML ~ x.lm.ML)
plot(detrend$data.Year,detrend$detrend_SAM)
print(coef(LM.ML))[1])
print(coef(LM.ML))[1]
print(coef(LM.ML)[1])
summary(LM.ML)$coeff[1,2]
View(data)
#find the y-intercepts for SAM
x.lm.SAM <- 1/detrend$detrend_SAM
y.lm.SAM <- detrend$detrend_d13C_SAM
LM.SAM <- lm(y.lm.SAM ~ x.lm.SAM)
print(coef(SAM.ML)[1])
LM.SAM <- lm(y.lm.SAM ~ x.lm.SAM)
print(coef(LM.SAM)[1])
summary(LM.SAM)$coeff[1,2]
print(coef(LM.ML)[1])
#plot
ggplot(detrend, aes(x = data.Year, y = detrend_d13C_ML)) +
geom_point(color = "red") +
geom_point(aes(data.Year, detrend_ML), color = "blue")
#plot
ggplot(detrend, aes(x = data.Year, y = detrend_d13C_ML)) +
geom_point(color = "red") +
geom_point(aes(data.Year, detrend_ML), color = "blue")
ggplot(detrend, aes(x = data.Year, y = detrend_d13C_SAM)) +
geom_point(color = "red") +
geom_point(aes(data.Year, detrend_SAM), color = "blue")
#method 2: cumulative for hydrogen
frac.cum.H <- (log((R.D.H[11]/R.D.H[1])/log(depth[11]/depth[1]))/10)+1
D.H <- c(-86.00, -84.70, -83.40, -81.70, -80.00, -78.70, -77.80,
-75.60, -73.50, -71.90, -72.10)
R.D.H <- (D.H/1000)+1
#method 1: sequential for hydrogen
frac.H <- vector(length= length(D.H))
for(i in 2:length(D.H)){
frac.H[i] <- log((R.D.H[i]/R.D.H[i-1])/log(depth[i]/depth[i-1]))+1
}
frac.H
#Set up vectors and then a data frame for the given numbers
depth <- seq(from = 5, to = 4, by = -0.1)
O18.O16 <- c(-12.00, -11.60, -11.20, -10.90, -10.30, -10.04,
-9.50, -9.00, -8.80, -8.20, -7.90)
D.H <- c(-86.00, -84.70, -83.40, -81.70, -80.00, -78.70, -77.80,
-75.60, -73.50, -71.90, -72.10)
R.D.H <- (D.H/1000)+1
#method 1: sequential for hydrogen
frac.H <- vector(length= length(D.H))
for(i in 2:length(D.H)){
frac.H[i] <- log((R.D.H[i]/R.D.H[i-1])/log(depth[i]/depth[i-1]))+1
}
frac.H
#method 1: sequential time intervals
frac <- vector(length = length(R.O18.O16))
for(i in 2:length(O18.O16)){
frac[i] <- log((R.O18.O16[i]/R.O18.O16[i-1]))/log(depth[i]/depth[i-1])+1
}
#Set up vectors and then a data frame for the given numbers
depth <- seq(from = 5, to = 4, by = -0.1)
function (..., list = character(), package = NULL, lib.loc = NULL,
verbose = getOption("verbose"), envir = .GlobalEnv, overwrite = TRUE)
knitr::opts_chunk$set(echo = TRUE)
#Set up vectors and then a data frame for the given numbers
depth <- seq(from = 5, to = 4, by = -0.1)
O18.O16 <- c(-12.00, -11.60, -11.20, -10.90, -10.30, -10.04,
-9.50, -9.00, -8.80, -8.20, -7.90)
R.O18.O16 <- (O18.O16/1000)+1
D.H <- c(-86.00, -84.70, -83.40, -81.70, -80.00, -78.70, -77.80,
-75.60, -73.50, -71.90, -72.10)
R.D.H <- (D.H/1000)+1
df <- data.frame(depth, O18.O16, D.H, R.O18.O16, R.D.H)
#method 1: sequential time intervals
frac <- vector(length = length(R.O18.O16))
for(i in 2:length(O18.O16)){
frac[i] <- log((R.O18.O16[i]/R.O18.O16[i-1]))/log(depth[i]/depth[i-1])+1
}
log((R.O18.O16[2]/R.O18.O16[1])/(depth[2]/depth[1]))+1
mean.frac <- mean(frac[2:11])
sd.frac <- sd(frac[2:11])
frac
mean.frac <- mean(frac[2:11])
mean.frac
#method 2: cumulative from initial condition
a <- R.O18.O16[11]/R.O18.O16[1]
b <- depth[11]/depth[1]
frac.cum <- (log(a)/log(b))+1 #divide by 10 because that is the number of total time steps that we are doing?
#convert frac values into delta notation
d.frac <- (mean.frac-1)*1000
d.frac
#method 1: sequential for hydrogen
frac.H <- vector(length= length(D.H))
for(i in 2:length(D.H)){
frac.H[i] <- log((R.D.H[i]/R.D.H[i-1]))/log(depth[i]/depth[i-1])+1
}
mean.frac.H <- mean(frac.H[2:11])
sd.frac.H <- sd(frac.H[2:11])
#method 2: cumulative for hydrogen
frac.cum.H <- (log((R.D.H[11]/R.D.H[1])/log(depth[11]/depth[1]))/10)+1
#method 2: cumulative for hydrogen
c <- R.D.H[11]/R.D.H[1]
d <- depth[11]/depth[1]
frac.cum.H <- ((log(c)/log(d))/10)+1
frac.cum.H
mean.frac.H
mean.frac
frac.cum
#method 1: sequential for hydrogen
frac.H <- vector(length= length(D.H))
for(i in 2:length(D.H)){
frac.H[i] <- log((R.D.H[i]/R.D.H[i-1]))/log(depth[i]/depth[i-1])+1
}
mean.frac.H <- mean(frac.H[2:11])
sd.frac.H <- sd(frac.H[2:11])
#method 2: cumulative for hydrogen
c <- R.D.H[11]/R.D.H[1]
d <- depth[11]/depth[1]
frac.cum.H <- ((log(c)/log(d))/10)+1
frac.cum.H
mean.frac.H
frac.H
sd.frac.H <- sd(frac.H[2:11])
sd.frac.H
R.D.H
frac.cum.H <- (log(c)/log(d))+1
<- mean(frac.H[2:11])
mean.frac.H
frac.cum.H
#convert frac values into delta notation
d.frac.H <- (mean.frac.H-1)*1000
d.frac.H
epsilon.O <- d.frac
epsilon.H <- d.frac.H
#Calculations of instantaneous product for oxygen
O18.O16 <- c(-12.00, -11.60, -11.20, -10.90, -10.30, -10.04,
-9.50, -9.00, -8.80, -8.20, -7.90)
inst.prod.O <- O18.O16 - epsilon.O
library(dplyr)
library(dplyr)
#Calculating cumulative product of O above the pool
cum.product.O <- cummean(inst.prod.O)
blank.correct <- read.csv(file = "~/Documents/GitHub/Isotope class/data/ProblemSet2/BlankCorrection.csv")
View(blank.correct)
blank.correct$Area.28[1]
blank.correct$Area.28[2]
M.I2 <- blank.correct$Area.28[1]
M.out <- blank.correct$Area.28[2]
M.out-M.I2
M.I2 <- blank.correct$Area.28[1]
M.out <- blank.correct$Area.28[2]
# use fraction 1 plus fraction 2 = 1 equation to solve for area under the curve if our sample had truly been JUST sample
M.I1 <- M.out-M.I2
M.I1
#Convert both delta notation values into R notation
R2 <- (4.315/1000)+1
R2
Rout <- (10.301/1000)+1
Rout
M.I1/M.out
#Use equation R = fI1*RI1 + fI2*RI2 to solve for the R value of the sample if there was not the confounding factor of atmospheric N
fI1 <- M.I1/M.out
#Use equation R = fI1*RI1 + fI2*RI2 to solve for the R value of the sample if there was not the confounding factor of atmospheric N
fI1 <- M.I1/M.out
fI2 <- M.I2/M.out
(Rout-(fI2*R2))/fI1
trueRsamp <- (Rout-(fI2*R2))/fI1
#convert the corrected Rsamp to delta
truedelta <- (trueRsamp*1000) + 1
truedelta
trueRsamp
#convert the corrected Rsamp to delta
truedelta <- (trueRsamp + 1) * 1000
truedelta
#convert the corrected Rsamp to delta
truedelta <- (trueRsamp 1 1) * 1000
truedelta
#convert the corrected Rsamp to delta
truedelta <- (trueRsamp - 1) * 1000
truedelta
#find the difference to figure out how much our original measurement was off by
diff <- truedelta - 10.301
diff
setwd("~/Documents/GitHub/CSIA_lab_work/data/final")
library(dplyr)
library(readr)
library(ggplot2)
library(ggpubr)
#read in the main data file
data <- read.csv(file="main.trophic.csv")
GLU.all <- ggplot(data = data, aes(x = Year, y = GLU.mean, color = System)) +
geom_point(size = 3, alpha = 0.7) +
labs(title = "Glutamic Acid (trophic) Signature Through Time",
x = "Year",
y = "GLU d15N") +
theme(axis.title = element_text(size = 15),
plot.title = element_text(size=16)) +
geom_vline(xintercept=1977, linetype ="dashed") +
geom_vline(xintercept=1998, linetype ="dashed") +
geom_smooth(aes(group=1))
GLU.all
#Run this file after running DriftCorrection.R, which corrects data
#but does not remove any outliers. This script will take the folder
#named "with_outliers" that is produced in the previous script, and
#compiles those scripts to remove duplicates.
#This script also calculates trophic position and adds a TP column to
#the dataset. The output of this script is a file called "all.data.csv"
#which can be found in the "final" folder.
setwd("~/Documents/GitHub/CSIA_lab_work/data")
rm(list = ls())
library(dplyr)
library(readr)
#compile all the csv files to make one dataframe of all data
df <- list.files(path=setwd("~/Documents/GitHub/CSIA_lab_work/data/with_outliers")) %>%
lapply(read_csv) %>%
bind_rows
#compile all the csv files to make one dataframe of all data
df <- list.files(path=setwd("~/Documents/GitHub/CSIA_lab_work/data/with_outliers")) %>%
lapply(read_csv) %>%
bind_rows
#function to average duplicate/replicates and replace in data file with new averages
#run this function as many times as replicates there are
rm_duplicates <- function(df, ID, Year, System, Age){
a <- subset(df, new.ID == ID)
b <- a[,6:15]
vec <- vector(mode="numeric", length=10)
for(i in 1:10){
vec[i] <- mean(as.numeric(b[,i]))
}
c <- append(c(1, ID, Year, System, Age),c(vec, ID, 0))
norep <- df[!df$new.ID==ID,]
new.data <- rbind(norep, c)
print(new.data)
}
data <- rm_duplicates(df = df, ID = "01_E_3", Year = "2001", System = "Egegik", Age = "3")
View(df)
rm(list = ls())
setwd("~/Documents/GitHub/CSIA_lab_work/data")
library(dplyr)
#Run this file after running DriftCorrection.R, which corrects data
#but does not remove any outliers. This script will take the folder
#named "with_outliers" that is produced in the previous script, and
#compiles those scripts to remove duplicates.
#This script also calculates trophic position and adds a TP column to
#the dataset. The output of this script is a file called "all.data.csv"
#which can be found in the "final" folder.
setwd("~/Documents/GitHub/CSIA_lab_work/data")
rm(list = ls())
library(dplyr)
library(readr)
#compile all the csv files to make one dataframe of all data
df <- list.files(path=setwd("~/Documents/GitHub/CSIA_lab_work/data/with_outliers")) %>%
lapply(read_csv) %>%
bind_rows
#compile all the csv files to make one dataframe of all data
df <- list.files(path=setwd("~/Documents/GitHub/CSIA_lab_work/data/with_outliers")) %>%
lapply(read_csv) %>%
bind_rows
df <- df[!df$AAID == "REF",]
View(df)
df <- df[!df$Sample.ID == "5AA",]
df <- df[!c(1,13,14),]
View(df)
df <- df[!c(13,14),]
df <- df[!c(13:14),]
df <- df[,!13:14]
df <- df[,!c(13,14)]
df[,1:12]
a <- df[,1:12]
View(df)
View(a)
df <- df[,1:12]
View(df)
#####Add Year column ####
year.2digit <- substr(df$Sample.ID, 1, 2)
year <- vector(mode="character")
for(i in 1:length(year.2digit)){
if(year.2digit[i] <= 22){
year[i] <- paste0(20, year.2digit[i])
} else{
year[i] <- paste0(19, year.2digit[i])
}
}
df$Year <- year
df <- df %>% relocate(Year, .before = ALA.mean)
View(df)
df$Year <- year
df <- df %>% relocate(Year, .before = VAL.mean)
#####Add System column####
sys <- substr(df$Sample.ID, 4, 4)
system <- vector(mode="character")
for(i in 1:length(sys)){
if(sys[i] == "W"){
system[i] <- "Wood"
} else if(sys[i] == "K"){
system[i] <- "Kvichak"
}  else{
system[i] <- "Egegik"
}
}
df$System <- system
df <- df %>% relocate(System, .before = VAL.mean)
#####Add Age column####
df$Age <- substr(df$Sample.ID, 6, 6)
df <- df %>% relocate(Age, .before = VAL.mean)
#function to average duplicate/replicates and replace in data file with new averages
#run this function as many times as replicates there are
rm_duplicates <- function(df, ID, Year, System, Age){
a <- subset(df, new.ID == ID)
b <- a[,6:15]
vec <- vector(mode="numeric", length=10)
for(i in 1:10){
vec[i] <- mean(as.numeric(b[,i]))
}
c <- append(c(1, ID, Year, System, Age),c(vec, ID, 0))
norep <- df[!df$new.ID==ID,]
new.data <- rbind(norep, c)
print(new.data)
}
rm(a)
data <- rm_duplicates(df = df, ID = "01_E_3", Year = "2001", System = "Egegik", Age = "3")
substr(df$Sample.ID, 1, 6)
####Add column of replicates####
df$new.ID <- substr(df$Sample.ID, 1, 6)
substr(df$Sample.ID, 8, 8)
rep <- substr(df$Sample.ID, 8, 8)
df$rep <- substr(df$Sample.ID, 8, 8)
#function to average duplicate/replicates and replace in data file with new averages
#run this function as many times as replicates there are
rm_duplicates <- function(df, ID, Year, System, Age){
a <- subset(df, new.ID == ID)
b <- a[,6:15]
vec <- vector(mode="numeric", length=10)
for(i in 1:10){
vec[i] <- mean(as.numeric(b[,i]))
}
c <- append(c(1, ID, Year, System, Age),c(vec, ID, 0))
norep <- df[!df$new.ID==ID,]
new.data <- rbind(norep, c)
print(new.data)
}
data <- rm_duplicates(df = df, ID = "01_E_3", Year = "2001", System = "Egegik", Age = "3")
#function to average duplicate/replicates and replace in data file with new averages
#run this function as many times as replicates there are
rm_duplicates <- function(df, ID, Year, System, Age){
a <- subset(df, new.ID == ID)
b <- a[,6:15]
vec <- vector(mode="numeric", length=10)
for(i in 1:10){
vec[i] <- mean(as.numeric(b[,i]))
}
c <- append(c(1, ID, Year, System, Age),c(vec, ID, 0))
norep <- df[!df$new.ID==ID,]
new.data <- rbind(norep, c)
print(new.data)
}
data <- rm_duplicates(df = df, ID = "01_E_3", Year = "2001", System = "Egegik", Age = "3")
subset(df, new.ID == "01_E_3")
a <- subset(df, new.ID == "01_E_3")
b <- a[,6:15]
b
View(a)
vec <- vector(mode="numeric", length=10)
vec
for(i in 1:10){
vec[i] <- mean(as.numeric(b[,i]))
}
b[,1])
b[,1]
mean(b[,1])
as.numeric(b[,1])
b
as.data.frame(a[,6:15])
b <- as.data.frame(a[,6:15])
vec <- vector(mode="numeric", length=10)
for(i in 1:10){
vec[i] <- mean(as.numeric(b[,i]))
}
vec
#function to average duplicate/replicates and replace in data file with new averages
#run this function as many times as replicates there are
rm_duplicates <- function(df, ID, Year, System, Age){
a <- subset(df, new.ID == ID)
b <- as.data.frame(a[,6:15])
vec <- vector(mode="numeric", length=10)
for(i in 1:10){
vec[i] <- mean(as.numeric(b[,i]))
}
c <- append(c(1, ID, Year, System, Age),c(vec, ID, 0))
norep <- df[!df$new.ID==ID,]
new.data <- rbind(norep, c)
print(new.data)
}
rm(a)
rm(b)
df <- rm_duplicates(df = df, ID = "01_E_3", Year = "2001", System = "Egegik", Age = "3")
df <- rm_duplicates(df = df, ID = "22_K_3", Year = "2022", System = "Kvichak", Age = "3")
df <- rm_duplicates(df = df, ID = "22_W_3", Year = "2022", System = "Wood", Age = "3")
df <- rm_duplicates(df = df, ID = "13_W_2", Year = "2013", System = "Wood", Age = "2")
df <- rm_duplicates(df = df, ID = "13_W_3", Year = "2013", System = "Wood", Age = "3")
df <- rm_duplicates(df = df, ID = "04_E_2", Year = "2004", System = "Egegik", Age = "2")
df <- rm_duplicates(df = df, ID = "10_W_2", Year = "2010", System = "Wood", Age = "2")
df <- rm_duplicates(df = df, ID = "22_E_2", Year = "2022", System = "Egegik", Age = "2")
df <- rm_duplicates(df = df, ID = "74_W_2", Year = "1974", System = "Wood", Age = "2")
df <- rm_duplicates(df = df, ID = "89_K_2", Year = "1989", System = "Kvichak", Age = "2")
df <- rm_duplicates(df = df, ID = "89_W_2", Year = "1989", System = "Wood", Age = "2")
View(df)
#check to see if there are any duplicate samples left
anyDuplicated(data$new.ID)
#check to see if there are any duplicate samples left
anyDuplicated(df$new.ID)
df[75,]
df <- rm_duplicates(df = df, ID = "07_K_2", Year = "2007", System = "Kvichak", Age = "2")
#check to see if there are any duplicate samples left
anyDuplicated(df$new.ID)
df[74,]
df <- rm_duplicates(df = df, ID = "74_E_2", Year = "1974", System = "Egegik", Age = "2")
#check to see if there are any duplicate samples left
anyDuplicated(df$new.ID)
#remove the last two columns
main.data <- df
main.data <- main.data[,1:15]
View(main.data)
#remove the last two columns
data <- df
#remove the last two columns
data <- df
data <- data[,1:15]
#define beta and TDF values, this can be changed later if necessary
beta <- 3.4 #commonly used constant
TDF <- 7.06 #from Lerner et al 2020
#make an empty data frame to fill with Sample.ID and trophic position
tp <- data.frame(matrix(nrow = length(data$Sample.ID), ncol = 2))
tp <-setNames(tp, c("Sample.ID","Trophic.Position"))
#for loop to calculate trophic position and fill data frame
for(i in 1:length(data$Sample.ID)){
tp[i,2] <- 1+ ((data$GLU.mean[i]-data$PHE.mean[i]-beta)/TDF)
tp[i,1] <- data$Sample.ID[i]
}
View(tp)
tp <-setNames(tp, c("Sample.ID","Trophic.Position"))
data <- as.data.frame(data[,1:15])
#for loop to calculate trophic position and fill data frame
for(i in 1:length(data$Sample.ID)){
tp[i,2] <- 1 + ((data$GLU.mean[i]-data$PHE.mean[i]-beta)/TDF)
tp[i,1] <- data$Sample.ID[i]
}
View(tp)
length(data$Sample.ID)
data$GLU.mean[1]
(as.numeric(data$GLU.mean[1])
(as.numeric(data$GLU.mean[1])
(as.numeric(data$GLU.mean[1])
1 + ((as.numeric(data$GLU.mean[1])-as.numeric(data$PHE.mean[1])-beta)/TDF)
data$Sample.ID[1]
#for loop to calculate trophic position and fill data frame
for(i in 1:length(data$Sample.ID)){
tp[i,2] <- 1 + ((as.numeric(data$GLU.mean[i])-as.numeric(data$PHE.mean[i])-beta)/TDF)
tp[i,1] <- data$Sample.ID[i]
}
tp
#combine new data frame with original
data <- cbind(data, tp)
data <- main.trophic[, 3:18]
data <- data[, 3:18]
View(data)
data <- data[, 3:17]
View(data)
#place holder so I can look at this data before I figure out these reps
file.name <- "~/Documents/GitHub/CSIA_lab_work/data/final/all.data.csv"
write.csv(data, file.name)
#this is an r script to do some basic data visualization of the cleaned data
#at this point the data should be drift corrected, compiled, and had all
#duplicates and replicates removed and averaged
rm(list =ls())
function (..., list = character(), package = NULL, lib.loc = NULL,
verbose = getOption("verbose"), envir = .GlobalEnv, overwrite = TRUE)
