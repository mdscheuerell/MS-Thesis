# Print the combined plot
print(combined_plot)
# Combine the two plots with separate y-axes
combined_plot <- plot_co2 +
geom_point(aes(y = d13C_ML), col = "green") +
geom_point(aes(y = d13C_SAM), col = "purple") +
scale_y_continuous(
name = "CO2",
sec.axis = sec_axis(~., name = "d13C", labels = scales::number_format())
)
# Print the combined plot
print(combined_plot)
library(tidyverse)
library(tidyverse)
#finding the mean and standard deviation within each year, make a new data frame with this infro
annual_data <- data %>%
group_by(Year) %>%
summarise(
mean.CO2.ML = mean(CO2_ML),
annual.sd.CO2.ML = sd(CO2_ML),
mean.CO2.SAM = mean(CO2_SAM),
annual.sd.CO2.SAM = sd(CO2_SAM),
mean.d13C.ML = mean(d13C_ML),
annual.sd.d13C.ML = sd(d13C_ML),
mean.d13C.SAM = mean(d13C_SAM),
annual.sd.d13C.SAM = sd(d13C_SAM))
#find the difference between each time step
diff.CO2.ML <- vector(length = 39)
for(i in 1:39){
diff.CO2.ML[i] <- annual_data$mean.CO2.ML[i+1]-annual_data$mean.CO2.ML[i]
}
diff.CO2.SAM <- vector(length = 39)
for(i in 1:39){
diff.CO2.SAM[i] <- annual_data$mean.CO2.SAM[i+1]-annual_data$mean.CO2.SAM[i]
}
diff.d13C_ML <- vector(length = 39)
for(i in 1:39){
diff.d13C_ML[i] <- annual_data$mean.d13C.ML[i+1]-annual_data$mean.d13C.ML[i]
}
diff.d13C_SAM <- vector(length = 39)
for(i in 1:39){
diff.d13C_SAM[i] <- annual_data$mean.d13C.SAM[i+1]-annual_data$mean.d13C.SAM[i]
}
#average each set of differences to find the average change per year
mean(diff.CO2.ML[1:38])
mean(diff.CO2.SAM[1:38])
mean(diff.d13C_ML[1:38])
mean(diff.d13C_SAM[1:38])
```
print(coef(sourceLM.ML)[1]) # this is the y-intercept which is the source d13C value at ML
#ML
y.axis.ML <- data$d13C_ML
x.axis.ML <- 1/(data$CO2_ML)
sourceLM.ML <- lm((y.axis.ML ~
x.axis.ML))
print(coef(sourceLM.ML)[1]) # this is the y-intercept which is the source d13C value at ML
summary(sourceLM.ML)$coeff[1,2] #this is standard error of the y-intercept
#SAM
y.axis.SAM <- data$d13C_SAM
x.axis.SAM <- 1/data$CO2_SAM
sourceLM.SAM <- lm((y.axis.SAM ~ x.axis.SAM))
print(coef(sourceLM.SAM)[1]) # this is the y-intercept which is the source d13C value at SAM
summary(sourceLM.SAM)$coeff[1,2] #this is standard error of the y-intercept
#mean of each year subtract this from each data point
data$annual_mean_CO2_ML <- annual_data$mean.CO2.ML[match(data$Year, annual_data$Year)]
View(data)
data$annual_mean_d13C_ML <- annual_data$mean.d13C.ML[match(data$Year, annual_data$Year)]
data$annual_mean_CO2_SAM <- annual_data$mean.CO2.SAM[match(data$Year, annual_data$Year)]
data$annual_mean_d13C_SAM <- annual_data$mean.d13C.SAM[match(data$Year, annual_data$Year)]
ML_CO2 <- data$CO2_ML - data$annual_mean_CO2_ML
ML_CO2
ML_d13C <- data$d13C_ML - data$annual_mean_d13C_ML
ML_d13C
#take the original data point and subtract the mean
#creates vectors of data that are detrended
detrend_ML <- data$CO2_ML - data$annual_mean_CO2_ML
detrend_SAM <- data$CO2_SAM - data$annual_mean_CO2_SAM
detrend_d13C_ML <- data$d13C_ML - data$annual_mean_d13C_ML
detrend_d13C_SAM <- data$d13C_ML - data$annual_mean_d13C_SAM
#make a new data frame with the detrended data
detrend <- data.frame(detrend_d13C_ML, detrend_ML, detrend_d13C_SAM,detrend_SAM, data$Year)
#plot
ggplot(detrend, aes(x = data.Year, y = detrend_d13C_ML)) +
geom_point(color = "red") +
geom_point(aes(data.Year, detrend_ML), color = "blue")
ggplot(detrend, aes(x = data.Year, y = detrend_d13C_SAM)) +
geom_point(color = "red") +
geom_point(aes(data.Year, detrend_SAM), color = "blue")
View(detrend)
#plot for ML
ggplot(detrend, aes(x = data.Year, y = detrend_d13C_ML)) +
geom_point(color = "red") +
geom_point(aes(data.Year, detrend_ML))
x.lm.ML <- 1/detrend$detrend_ML
y.lm.ML <- detrend$detrend_d13C_ML
ggplot(detrend, aes(x = data.Year, y = detrend_d13C_SAM)) +
geom_point(color = "red") +
geom_point(aes(data.Year, detrend_SAM), color = "blue")
x.lm.ML <- 1/detrend$detrend_ML
y.lm.ML <- detrend$detrend_d13C_ML
LM.ML <- lm(y.lm.ML ~ x.lm.ML)
plot(detrend$data.Year,detrend$detrend_SAM)
print(coef(LM.ML))[1])
print(coef(LM.ML))[1]
print(coef(LM.ML)[1])
summary(LM.ML)$coeff[1,2]
View(data)
#find the y-intercepts for SAM
x.lm.SAM <- 1/detrend$detrend_SAM
y.lm.SAM <- detrend$detrend_d13C_SAM
LM.SAM <- lm(y.lm.SAM ~ x.lm.SAM)
print(coef(SAM.ML)[1])
LM.SAM <- lm(y.lm.SAM ~ x.lm.SAM)
print(coef(LM.SAM)[1])
summary(LM.SAM)$coeff[1,2]
print(coef(LM.ML)[1])
#plot
ggplot(detrend, aes(x = data.Year, y = detrend_d13C_ML)) +
geom_point(color = "red") +
geom_point(aes(data.Year, detrend_ML), color = "blue")
#plot
ggplot(detrend, aes(x = data.Year, y = detrend_d13C_ML)) +
geom_point(color = "red") +
geom_point(aes(data.Year, detrend_ML), color = "blue")
ggplot(detrend, aes(x = data.Year, y = detrend_d13C_SAM)) +
geom_point(color = "red") +
geom_point(aes(data.Year, detrend_SAM), color = "blue")
#method 2: cumulative for hydrogen
frac.cum.H <- (log((R.D.H[11]/R.D.H[1])/log(depth[11]/depth[1]))/10)+1
D.H <- c(-86.00, -84.70, -83.40, -81.70, -80.00, -78.70, -77.80,
-75.60, -73.50, -71.90, -72.10)
R.D.H <- (D.H/1000)+1
#method 1: sequential for hydrogen
frac.H <- vector(length= length(D.H))
for(i in 2:length(D.H)){
frac.H[i] <- log((R.D.H[i]/R.D.H[i-1])/log(depth[i]/depth[i-1]))+1
}
frac.H
#Set up vectors and then a data frame for the given numbers
depth <- seq(from = 5, to = 4, by = -0.1)
O18.O16 <- c(-12.00, -11.60, -11.20, -10.90, -10.30, -10.04,
-9.50, -9.00, -8.80, -8.20, -7.90)
D.H <- c(-86.00, -84.70, -83.40, -81.70, -80.00, -78.70, -77.80,
-75.60, -73.50, -71.90, -72.10)
R.D.H <- (D.H/1000)+1
#method 1: sequential for hydrogen
frac.H <- vector(length= length(D.H))
for(i in 2:length(D.H)){
frac.H[i] <- log((R.D.H[i]/R.D.H[i-1])/log(depth[i]/depth[i-1]))+1
}
frac.H
#method 1: sequential time intervals
frac <- vector(length = length(R.O18.O16))
for(i in 2:length(O18.O16)){
frac[i] <- log((R.O18.O16[i]/R.O18.O16[i-1]))/log(depth[i]/depth[i-1])+1
}
#Set up vectors and then a data frame for the given numbers
depth <- seq(from = 5, to = 4, by = -0.1)
function (..., list = character(), package = NULL, lib.loc = NULL,
verbose = getOption("verbose"), envir = .GlobalEnv, overwrite = TRUE)
knitr::opts_chunk$set(echo = TRUE)
#Set up vectors and then a data frame for the given numbers
depth <- seq(from = 5, to = 4, by = -0.1)
O18.O16 <- c(-12.00, -11.60, -11.20, -10.90, -10.30, -10.04,
-9.50, -9.00, -8.80, -8.20, -7.90)
R.O18.O16 <- (O18.O16/1000)+1
D.H <- c(-86.00, -84.70, -83.40, -81.70, -80.00, -78.70, -77.80,
-75.60, -73.50, -71.90, -72.10)
R.D.H <- (D.H/1000)+1
df <- data.frame(depth, O18.O16, D.H, R.O18.O16, R.D.H)
#method 1: sequential time intervals
frac <- vector(length = length(R.O18.O16))
for(i in 2:length(O18.O16)){
frac[i] <- log((R.O18.O16[i]/R.O18.O16[i-1]))/log(depth[i]/depth[i-1])+1
}
log((R.O18.O16[2]/R.O18.O16[1])/(depth[2]/depth[1]))+1
mean.frac <- mean(frac[2:11])
sd.frac <- sd(frac[2:11])
frac
mean.frac <- mean(frac[2:11])
mean.frac
#method 2: cumulative from initial condition
a <- R.O18.O16[11]/R.O18.O16[1]
b <- depth[11]/depth[1]
frac.cum <- (log(a)/log(b))+1 #divide by 10 because that is the number of total time steps that we are doing?
#convert frac values into delta notation
d.frac <- (mean.frac-1)*1000
d.frac
#method 1: sequential for hydrogen
frac.H <- vector(length= length(D.H))
for(i in 2:length(D.H)){
frac.H[i] <- log((R.D.H[i]/R.D.H[i-1]))/log(depth[i]/depth[i-1])+1
}
mean.frac.H <- mean(frac.H[2:11])
sd.frac.H <- sd(frac.H[2:11])
#method 2: cumulative for hydrogen
frac.cum.H <- (log((R.D.H[11]/R.D.H[1])/log(depth[11]/depth[1]))/10)+1
#method 2: cumulative for hydrogen
c <- R.D.H[11]/R.D.H[1]
d <- depth[11]/depth[1]
frac.cum.H <- ((log(c)/log(d))/10)+1
frac.cum.H
mean.frac.H
mean.frac
frac.cum
#method 1: sequential for hydrogen
frac.H <- vector(length= length(D.H))
for(i in 2:length(D.H)){
frac.H[i] <- log((R.D.H[i]/R.D.H[i-1]))/log(depth[i]/depth[i-1])+1
}
mean.frac.H <- mean(frac.H[2:11])
sd.frac.H <- sd(frac.H[2:11])
#method 2: cumulative for hydrogen
c <- R.D.H[11]/R.D.H[1]
d <- depth[11]/depth[1]
frac.cum.H <- ((log(c)/log(d))/10)+1
frac.cum.H
mean.frac.H
frac.H
sd.frac.H <- sd(frac.H[2:11])
sd.frac.H
R.D.H
frac.cum.H <- (log(c)/log(d))+1
<- mean(frac.H[2:11])
mean.frac.H
frac.cum.H
#convert frac values into delta notation
d.frac.H <- (mean.frac.H-1)*1000
d.frac.H
epsilon.O <- d.frac
epsilon.H <- d.frac.H
#Calculations of instantaneous product for oxygen
O18.O16 <- c(-12.00, -11.60, -11.20, -10.90, -10.30, -10.04,
-9.50, -9.00, -8.80, -8.20, -7.90)
inst.prod.O <- O18.O16 - epsilon.O
library(dplyr)
library(dplyr)
#Calculating cumulative product of O above the pool
cum.product.O <- cummean(inst.prod.O)
blank.correct <- read.csv(file = "~/Documents/GitHub/Isotope class/data/ProblemSet2/BlankCorrection.csv")
View(blank.correct)
blank.correct$Area.28[1]
blank.correct$Area.28[2]
M.I2 <- blank.correct$Area.28[1]
M.out <- blank.correct$Area.28[2]
M.out-M.I2
M.I2 <- blank.correct$Area.28[1]
M.out <- blank.correct$Area.28[2]
# use fraction 1 plus fraction 2 = 1 equation to solve for area under the curve if our sample had truly been JUST sample
M.I1 <- M.out-M.I2
M.I1
#Convert both delta notation values into R notation
R2 <- (4.315/1000)+1
R2
Rout <- (10.301/1000)+1
Rout
M.I1/M.out
#Use equation R = fI1*RI1 + fI2*RI2 to solve for the R value of the sample if there was not the confounding factor of atmospheric N
fI1 <- M.I1/M.out
#Use equation R = fI1*RI1 + fI2*RI2 to solve for the R value of the sample if there was not the confounding factor of atmospheric N
fI1 <- M.I1/M.out
fI2 <- M.I2/M.out
(Rout-(fI2*R2))/fI1
trueRsamp <- (Rout-(fI2*R2))/fI1
#convert the corrected Rsamp to delta
truedelta <- (trueRsamp*1000) + 1
truedelta
trueRsamp
#convert the corrected Rsamp to delta
truedelta <- (trueRsamp + 1) * 1000
truedelta
#convert the corrected Rsamp to delta
truedelta <- (trueRsamp 1 1) * 1000
truedelta
#convert the corrected Rsamp to delta
truedelta <- (trueRsamp - 1) * 1000
truedelta
#find the difference to figure out how much our original measurement was off by
diff <- truedelta - 10.301
diff
setwd("~/Documents/GitHub/CSIA_lab_work/data")
library(dplyr)
name <- c("Analysis", "ID1", "RT", "AreaAll", "d29N", "d15N", "AAID")
#This is the stable isotpe ratios of the internal and external standards.
# MAKE SURE THIS IS UP TO DATE BASED ON HEEL STANDARDS!!! These values are as of 01/23/2020.
#If your samples were esterified after 01/23/2020 these values should be verfied with the standard file on the HEEL drive
ALA <- -1.21
VAL <- 0.361
NOR <- 14.163
PHE <- -5.004
GLU <- -3.336
#Reading in the .csv of the NACHO data file and setting the file name for your output file
data.1 <- SL.1 <- read.csv("cleaned/20240207_GHenry_CSIA.csv") #modify with name of your data file
colnames(data.1)<-name
file.name <- "outliers_removed/20240207_outliersRem.csv" #file name for output file including relative file path
#### Correct to international standard of N air ####
#Calculations of offset values were done in R script "Correct_to_Nair.R"
#Three EA runs were looked at, the second was chosen as the most representative to base corrections off
#No linear relationship was found between offset and measured value so one average value
#will be applied to raw data
#The offset values were calculated as EA measured d15N - reference
offset <- mean(c(0.40160, 0.47160, 0.41725))
data.1$d15N.correct <- data.1$d15N - offset
data.1STD <- subset(data.1, ID1=="5AA") #get only the standard data
AA <- unique(unlist(data.1STD$AAID)) #make a list of the AAs in the data
Intercept<-data.frame(Intercept=rep(NA,length(AA))) #initiate a dataframe for the intercepts of the linear model
for(i in 1:length(AA)){
data <- subset(data.1STD, AAID==AA[i])
Intercept[i,1]<- coef(summary(lm(as.numeric(d15N.correct)~as.numeric(Analysis), data=data)))[1,1]
}
Intercept #intercept values looped by aa
Slope<-data.frame(Slope=rep(NA,length(AA))) #initiate a dataframe for the slopes of the linear model
for(i in 1:length(AA)){
data <- subset(data.1STD, AAID==AA[i])
Slope[i,1]<- coef(summary(lm(as.numeric(d15N.correct)~as.numeric(Analysis), data=data)))[2,1]
}
Slope #slope values looped by aa
Coef<- data.frame(AA, Intercept, Slope) #creating a dataframe of the slope and intercepts values for each AA
actual <- ifelse(data.1$AAID=="NOR", NOR,
ifelse(data.1$AAID=="ALA", ALA,
ifelse(data.1$AAID=="VAL", VAL,
ifelse(data.1$AAID=="PHE", PHE,
ifelse(data.1$AAID=="GLU", GLU,0)))))
actual #check your data -- if there are 0s than you have an AA that is not included in the standard 12AA mix and the code will need
slope <- ifelse(data.1$AAID=="NOR", filter(Coef, AA=="NOR")[1,3],
ifelse(data.1$AAID=="ALA", filter(Coef, AA=="ALA")[1,3],
ifelse(data.1$AAID=="VAL", filter(Coef, AA=="VAL")[1,3],
ifelse(data.1$AAID=="GLU", filter(Coef, AA=="GLU")[1,3],
ifelse(data.1$AAID=="PHE", filter(Coef, AA=="PHE")[1,3], 0)))))
intercept <-   ifelse(data.1$AAID=="NOR", filter(Coef, AA=="NOR")[1,2],
ifelse(data.1$AAID=="ALA", filter(Coef, AA=="ALA")[1,2],
ifelse(data.1$AAID=="VAL", filter(Coef, AA=="VAL")[1,2],
ifelse(data.1$AAID=="GLU", filter(Coef, AA=="GLU")[1,2],
ifelse(data.1$AAID=="PHE", filter(Coef, AA=="PHE")[1,2], 0)))))
#####Applying Drift Correction####
difference <- actual-(data.1$Analysis*slope+intercept) #Applying both a drift and step correction in on estep from linear model data
adj <- data.1$d15N.correct + difference
data <- cbind(data.1, adj)
View(data)
data
#####Remove all 5AA samples from dataset####
data <- subset(data, !ID1 == "5AA")
sample.ID <- unique(data$ID1)
sample.ID
values <- data[data$AAID == "PHE" & data$ID1 == sample.ID[1], "adj"]
values #look at values and determine if there are outliers
values <- data[data$AAID == "PHE" & data$ID1 == sample.ID[2], "adj"]
values #look at values and determine if there are outliers
values <- data[data$AAID == "PHE" & data$ID1 == sample.ID[3], "adj"]
values #look at values and determine if there are outliers
#remove the outlier row, only run this line of code when there is an outlier to remove
data <- data[!data$adj == values[3],]
values <- data[data$AAID == "PHE" & data$ID1 == sample.ID[3], "adj"]
values #look at values and determine if there are outliers
values <- data[data$AAID == "PHE" & data$ID1 == sample.ID[4], "adj"]
values #look at values and determine if there are outliers
values <- data[data$AAID == "PHE" & data$ID1 == sample.ID[5], "adj"]
values #look at values and determine if there are outliers
#do the sample process with GLU
values <- data[data$AAID == "GLU" & data$ID1 == sample.ID[1], "adj"]
values #look at values and determine if there are outliers
#do the sample process with GLU
values <- data[data$AAID == "GLU" & data$ID1 == sample.ID[2], "adj"]
values #look at values and determine if there are outliers
#do the sample process with GLU
values <- data[data$AAID == "GLU" & data$ID1 == sample.ID[3], "adj"]
values #look at values and determine if there are outliers
#do the sample process with GLU
values <- data[data$AAID == "GLU" & data$ID1 == sample.ID[4], "adj"]
values #look at values and determine if there are outliers
#do the sample process with GLU
values <- data[data$AAID == "GLU" & data$ID1 == sample.ID[5], "adj"]
values #look at values and determine if there are outliers
#remove the outlier row, only run this line of code when there is an outlier to remove
data <- data[!data$adj == values[2],]
#generate .csv file of this data with no outliers
write.csv(data, file = file.name)
#this script should be run after drift correcting and removing outliers
#from each individual run using the DriftCorrection_outliers.R script
setwd("~/Documents/GitHub/CSIA_lab_work/data")
library(dplyr)
library(readr)
#compile all the csv files to make one dataframe of all data
df <- list.files(path=setwd("~/Documents/GitHub/CSIA_lab_work/data/outliers_removed")) %>%
lapply(read_csv) %>%
bind_rows
#compile all the csv files to make one dataframe of all data
df <- list.files(path=setwd("~/Documents/GitHub/CSIA_lab_work/data/outliers_removed")) %>%
lapply(read_csv) %>%
bind_rows
df <- df[!df$AAID == "REF",]
View(df)
AA<- unique(unlist(df$AAID)) #make a list of the AAs in the data
mean <- aggregate(df['adj'], by = list(df$ID1, df$AAID), mean)
mean_names <- c("Sample.ID",paste0(AA,".mean"))
meanfull<-data.frame(matrix(0, nrow = length(unique(mean$Group.1)), ncol = length(AA)+1)) #initiate a dataframe for the intercepts of the linear model
colnames(meanfull) <- mean_names
meanfull[1:length(unique(mean$Group.1)),1]<- unique(mean$Group.1)
for(i in 1:length(AA)){
meanfull[1:length(unique(mean$Group.1)),i+1]<- mean%>%
filter(Group.2  == AA[i])%>%
select(adj)
}
meanfull #ALWAYS check to make sure everything looks right! small errors can break the code
sd<- aggregate(df['adj'], by = list(df$ID1, df$AAID), sd)
sd_names <- c("Sample.ID",paste0(AA,".sd"))
sdfull<-data.frame(matrix(0, nrow = length(unique(sd$Group.1)), ncol = length(AA)+1))
colnames(sdfull)<- sd_names
sdfull[1:length(unique(sd$Group.1)),1]<- unique(sd$Group.1)
for(i in 1:length(AA)){
sdfull[1:length(unique(sd$Group.1)),i+1]<- sd%>%
filter(Group.2  == AA[i])%>%
select(adj)
}
sdfull
Corrected <- merge(meanfull,sdfull, by="Sample.ID") #this merges the columns in both the SD and mean dataframes
Corrected
View(Corrected)
#####Add Year column ####
year.2digit <- substr(Corrected$Sample.ID, 1, 2)
year <- vector(mode="character")
for(i in 1:length(year.2digit)){
if(year.2digit[i] <= 22){
year[i] <- paste0(20, year.2digit[i])
} else{
year[i] <- paste0(19, year.2digit[i])
}
}
Corrected$Year <- year
Corrected <- Corrected %>% relocate(Year, .before = ALA.mean)
#####Add System column####
sys <- substr(Corrected$Sample.ID, 4, 4)
system <- vector(mode="character")
for(i in 1:length(sys)){
if(sys[i] == "W"){
system[i] <- "Wood"
} else if(sys[i] == "K"){
system[i] <- "Kvichak"
}  else{
system[i] <- "Egegik"
}
}
Corrected$System <- system
Corrected <- Corrected %>% relocate(System, .before = ALA.mean)
#####Add Age column####
Corrected$Age <- substr(Corrected$Sample.ID, 6, 6)
Corrected <- Corrected %>% relocate(Age, .before = ALA.mean)
#####Write new .csv file that has this clean data with no outliers####
setwd("~/Documents/GitHub/CSIA_lab_work/data")
View(Corrected)
file.name <- "final/main.clean.csv"
write.csv(Corrected, file = file.name)
View(Corrected)
#code to average all the duplicate samples and add the new average to the main
#data file. This starts with a csv file produced by the
#ConsolidateTriplicates.R script.
rm(list)
#code to average all the duplicate samples and add the new average to the main
#data file. This starts with a csv file produced by the
#ConsolidateTriplicates.R script.
rm(list = ls())
setwd("~/Documents/GitHub/CSIA_lab_work/data/final")
data <- read.csv(file="main.clean.csv")
source("~/Documents/GitHub/CSIA_lab_work/code/Duplicates.R", echo=TRUE)
#take a look at all replicate values
#make a new column with only the first chunk of the sample ID
#after this they can be removed
data$new.ID <- substr(data$Sample.ID, 1, 6)
rep <- substr(data$Sample.ID, 8, 8)
data$rep <- substr(data$Sample.ID, 8, 8)
which(data$rep == "R")
data[3,]
E013 <- subset(data, new.ID == "01_E_3")
data[9,]
W042<- subset(data, new.ID == "04_E_2")
data[22,]
E102 <- subset(data, new.ID == "10_W_2")
data[35,]
K222 <- subset(data, new.ID == "22_E_2")
data[40,]
K223 <- subset(data, new.ID == "22_K_3")
data[58,]
K742 <- subset(data, new.ID == "74_W_2")
data[66,]
K892 <- subset(data, new.ID == "89_K_2")
df <- rbind(E013,W042,E102,K222,K223,K742,K892)
View(df)
#write a new .csv file for the replicates to look at later if needed
file.name <- "~/Documents/GitHub/CSIA_lab_work/data/final/replicates.csv"
write.csv(df, file = file.name)
#function to average duplicate/replicates and replace in data file with new averages
#run this function as many times as replicates there are
rm_duplicates <- function(df, ID, Year, System, Age){
a <- subset(df, new.ID == ID)
b <- a[,6:15]
vec <- vector(mode="numeric", length=10)
for(i in 1:10){
vec[i] <- mean(as.numeric(b[,i]))
}
c <- append(c(1, ID, Year, System, Age),c(vec, ID, 0))
norep <- df[!df$new.ID==ID,]
new.data <- rbind(norep, c)
print(new.data)
}
#check to see if there are any duplicate samples left
anyDuplicated(data$new.ID)
View(data)
#code to average all the duplicate samples and add the new average to the main
#data file. This starts with a csv file produced by the
#ConsolidateTriplicates.R script.
rm(list = ls())
function (x, df1, df2, ncp, log = FALSE)
