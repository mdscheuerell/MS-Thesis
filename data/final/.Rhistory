nt <- rep(NA, length(nt.obs))
nt[1] <- N1
for(i in 2:length(nt)){
nt[i] <- nt[i - 1] + r * nt[i - 1] * (1 - (nt[i - 1]/K))
}
nll <- -sum(dpois(x = nt.obs, lambda = nt, log = TRUE))
return(nll)
}
#use optim to maximize nll
start.pars.dd <- c(0.25, 4, 135)
obs.fit.dd <- optim(par = start.pars.dd,
fn = obs.dd.nll,
nt.obs = nt.obs,
method = "BFGS") #density independent, observation based model
obs <- obs.fit.dd$value
obs
warnings()
obs
obs.dd.fit <- optim(par = start.pars.dd,
fn = obs.dd.nll,
nt.obs = nt.obs,
method = "BFGS") #density independent, observation based model
obs <- obs.dd.fit$value
obs
#calculating AIC
obs.AIC <- 2 * (obs + 3)
obs.AIC
#finding delta AIC value using old
process <- 86.020
#finding delta AIC value using old
process.AIC <- 86.020
obs.AIC
DAIC <- process.AIC - obs.AIC
DAIC
load("~/Documents/GitHub/FISH 454/Data/harborseals.rda")
View(harborseals)
#density independent
process.di.nll <- function(pars, nt){
r <- pars[1]
N0 <- pars[2]
nt.hat <- rep(NA, length(nt))
nt.hat[1] <- N0 + r*N0
for(i in 2:length(nt)){
nt.hat[i] <- nt[i-1] + r*nt[i-1]
}
nll <- -sum(dpois(x = nt, lambda = nt.hat, log = TRUE))
return(nll)
}
nt <- harborseals$abundance
nt <- harborseals$abundance
start.pars.dd <- c(0.25, 4000)
process.di.fit <- optim(par = start.pars,
fn = process.di.nll,
nt = nt,
method = "BFGS") #density independent, process based model
start.pars.di <- c(0.25, 4000)
process.di.fit <- optim(par = start.pars.di,
fn = process.di.nll,
nt = nt,
method = "BFGS") #density independent, process based model
process.di <- process.di.fit$value
process.di
process.di.fit
#density dependent
process.dd.nll <- function(pars, nt, K){
r <- pars[1]
N0 <- pars[2]
K <- pars[3]
nt.hat <- rep(NA, length(nt))
nt.hat[1] <- N0 + r * N0 * (1 - (N0/K))
for(i in 2:length(nt)){
nt.hat[i] <- nt[i-1] + r * nt[i-1] * (1 - (nt[i-1]/K))
}
nll <- -sum(dpois(x = nt, lambda = nt.hat, log = TRUE))
return(nll)
}
start.pars.dd <- c(0.25, 4, 3000)
start.pars.dd <- c(0.25, 3000, 3000)
process.dd.fit <- optim(par = start.pars.dd,
fn = process.dd.nll,
nt = nt,
K = K,
method = "BFGS") #density independent, process based model
process.dd <- process.dd.fit$value
process.dd
#calculate AIC for density independent
AIC.di.process <- 2 * (process.di + 2)
#calculate AIC for density dependent
AIC.dd.process <- 2 * (process.dd + 3)
#DAIC calculations
AIC.di - AIC.dd
#calculate AIC for density dependent
AIC.dd.process <- 2 * (process.dd + 3)
#DAIC calculations
AIC.di.process - AIC.dd.process
#DAIC calculations
AIC.dd.process - AIC.di.process
#### Observation Error Model ####
nt.obs <- wa_wolf$Abundance
#### Observation Error Model ####
nt.obs <- harborseals$abundance
#density dependent
obs.dd.nll <- function(pars, nt.obs) {
r <- pars[1]
N1 <- pars[2]
K <- pars[3]
nt <- rep(NA, length(nt.obs))
nt[1] <- N1
for(i in 2:length(nt)){
nt[i] <- nt[i - 1] + r * nt[i - 1] * (1 - (nt[i - 1]/K))
}
nll <- -sum(dpois(x = nt.obs, lambda = nt, log = TRUE))
return(nll)
}
start.pars.dd <- c(0.25, 4000, 4000)
obs.dd.fit <- optim(par = start.pars.dd,
fn = obs.dd.nll,
nt.obs = nt.obs,
method = "BFGS") #density independent, observation based model
obs.dd <- obs.dd.fit$value
obs.dd
#density independent
obs.di.nll <- function(pars, nt.obs) {
r <- pars[1]
N1 <- pars[2]
nt <- rep(NA, length(nt.obs))
nt[1] <- N1
for(i in 2:length(nt)){
nt[i] <- nt[i - 1] + r * nt[i - 1]
}
nll <- -sum(dpois(x = nt.obs, lambda = nt, log = TRUE))
return(nll)
}
start.pars.di <- c(0.25, 4000, 4000)
start.pars.di <- c(0.25, 4000)
start.pars.di <- c(0.25, 4000)
obs.di.fit <- optim(par = start.pars.di,
fn = obs.di.nll,
nt.obs = nt.obs,
method = "BFGS") #density independent, observation based model
obs.di <- obs.di.fit$value
obs.di
#calculate AIC for density independent
AIC.di.obs <- 2 * (obs.di + 2)
#calculate AIC for density dependent
AIC.dd.obs <- 2 * (obs.dd + 3)
#DAIC calculations
AIC.dd.obs - AIC.di.obs
#DAIC calculations
AIC.di.obs - AIC.dd.obs
obs.dd.nll(start.pars.di, nt.obs)
obs.dd.nll(start.pars.di, nt.obs)
obs.di.nll(start.pars.di, nt.obs)
start.pars.di <- c(1, 4000)
obs.di.nll(start.pars.di, nt.obs)
obs.di.nll(start.pars.di, nt.obs)
start.pars.di <- c(1, 4000)
obs.di.fit <- optim(par = start.pars.di,
fn = obs.di.nll,
nt.obs = nt.obs,
method = "BFGS") #density independent, observation based model
obs.di <- obs.di.fit$value
obs.di
#calculate AIC for density independent
AIC.di.obs <- 2 * (obs.di + 2)
#calculate AIC for density dependent
AIC.dd.obs <- 2 * (obs.dd + 3)
#DAIC calculations
AIC.di.obs - AIC.dd.obs
obs.di.fit <- optim(par = start.pars.di,
fn = obs.di.nll,
nt.obs = nt.obs,
method = "BFGS") #density independent, observation based model
obs.di
start.pars.di <- c(0.25, 4000)
obs.di.fit <- optim(par = start.pars.di,
fn = obs.di.nll,
nt.obs = nt.obs,
method = "BFGS") #density independent, observation based model
obs.di <- obs.di.fit$value
obs.di
obs.di.fit <- optim(par = start.pars.di,
fn = obs.di.nll,
nt.obs = nt.obs,
method = "BFGS") #density independent, observation based model
obs.di.fit <- optim(par = start.pars.di,
fn = obs.di.nll,
nt.obs = nt.obs,
method = "BFGS") #density independent, observation based model
obs.di <- obs.di.fit$value
#calculate AIC for density independent
AIC.di.obs <- 2 * (obs.di + 2)
#calculate AIC for density dependent
AIC.dd.obs <- 2 * (obs.dd + 3)
#DAIC calculations
AIC.di.obs - AIC.dd.obs
#density dependent
obs.dd.nll <- function(pars, nt.obs) {
r <- pars[1]
N1 <- pars[2]
K <- pars[3]
nt <- rep(NA, length(nt.obs))
nt[1] <- N1
for(i in 2:length(nt)){
nt[i] <- nt[i - 1] + r * nt[i - 1] * (1 - (nt[i - 1]/K))
}
nll <- -sum(dpois(x = nt.obs, lambda = nt, log = TRUE))
return(nll)
}
start.pars.dd <- c(0.25, 4000, 8000)
obs.dd.fit <- optim(par = start.pars.dd,
fn = obs.dd.nll,
nt.obs = nt.obs,
method = "BFGS") #density independent, observation based model
obs.dd <- obs.dd.fit$value
obs.dd
#calculate AIC for density independent
AIC.di.obs <- 2 * (obs.di + 2)
#calculate AIC for density dependent
AIC.dd.obs <- 2 * (obs.dd + 3)
#DAIC calculations
AIC.di.obs - AIC.dd.obs
process.dd.fit
process.dd.fit$par[1]
nt.hat <- rep(NA, length(nt))
#show the fits
r <- process.dd.fit$par[1]
N0 <- process.dd.fit$par[2]
K <- process.dd.fit$par[3]
nt.hat <- rep(NA, length(nt))
nt.hat
nt.hat[1] <- N0 + r * N0 * (1 - (N0/K))
nt.hat
for(i in 2:length(nt)){
nt.hat[i] <- nt[i-1] + r * nt[i-1] * (1 - (nt[i-1]/K))
}
nt.hat.dd <- rep(NA, length(nt))
nt.hat.dd[1] <- N0 + r * N0 * (1 - (N0/K))
for(i in 2:length(nt)){
nt.hat.dd[i] <- nt[i-1] + r * nt[i-1] * (1 - (nt[i-1]/K))
}
plot(x = nt.hat.dd,
y = nt)
plot(x = nt.hat.dd,
y = nt)
abline(lm(nt ~ nt.hat.dd), col = "red")
plot(x = harborseals$year,
y = nt.obs)
obs.di.fit
#showing data as plots
r <- obs.di.fit$par[1]
N1 <- obs.di.fit$par[2]
nt <- rep(NA, length(nt.obs))
nt[1] <- N1
for(i in 2:length(nt)){
nt[i] <- nt[i - 1] + r * nt[i - 1]
}
nt
plot(x = harborseals$year,
y = nt.obs)
lines(nt)
?lines
plot(x = harborseals$year,
y = nt.obs)
lines(x = harborseals$year,
y = nt, col = "red")
#showing data as plots
r.di <- obs.di.fit$par[1]
N1.di <- obs.di.fit$par[2]
nt.di <- rep(NA, length(nt.obs))
nt.di[1] <- N1.di
for(i in 2:length(nt)){
nt.di[i] <- nt.di[i - 1] + r.di * nt.di[i - 1]
}
proc.di.plot <- plot(x = harborseals$year,
y = nt.obs)
lines(x = harborseals$year,
y = nt.di, col = "red")
obs.dd.fit
obs.dd.fit$par[1]
r.dd <- obs.dd.fit$par[1]
N1.dd <- obs.dd.fit$par[2]
K.dd <- obs.dd.fit$par[3]
nt.dd <- rep(NA, length(nt.obs))
nt.dd[1] <- N1
for(i in 2:length(nt.dd)){
nt.dd[i] <- nt.dd[i - 1] + r.dd * nt.dd[i - 1] * (1 - (nt.dd[i - 1]/K.dd))
}
process.plot <- plot(x = harborseals$year,
y = nt.obs)
lines(x = harborseals$year,
y = nt.di, col = "red")
lines(x = harborseals$year,
y = nt.dd, col = "blue")
legend("topright",
legend = c("Density Independent", "Density Dependent"),
col = c("red", "blue"),
lty = 1,
cex = 1.2)
legend("topleft",
legend = c("Density Independent", "Density Dependent"),
col = c("red", "blue"),
lty = 1,
cex = 1.2)
process.plot <- plot(x = harborseals$year,
y = nt.obs)
lines(x = harborseals$year,
y = nt.di, col = "red")
lines(x = harborseals$year,
y = nt.dd, col = "blue")
legend("topleft",
legend = c("Density Independent", "Density Dependent"),
col = c("red", "blue"),
lty = 1,
cex = 1.2)
process.plot <- plot(x = harborseals$year,
y = nt.obs,
xlab = "Year", ylab = "Population")
lines(x = harborseals$year,
y = nt.di, col = "red")
lines(x = harborseals$year,
y = nt.dd, col = "blue")
legend("topleft",
legend = c("Density Independent", "Density Dependent"),
col = c("red", "blue"),
lty = 1,
cex = 1.2)
observation.plot <- plot(x = harborseals$year,
y = nt.obs,
xlab = "Year", ylab = "Population")
lines(x = harborseals$year,
y = nt.di, col = "red")
lines(x = harborseals$year,
y = nt.dd, col = "blue")
legend("topleft",
legend = c("Density Independent", "Density Dependent"),
col = c("red", "blue"),
lty = 1,
cex = 1.2)
setwd("~/Documents/GitHub/CSIA_lab_work/data")
rm(list = ls())
library(dplyr)
library(readr)
#compile all the csv files to make one dataframe of all data
df <- list.files(path=setwd("~/Documents/GitHub/CSIA_lab_work/data/with_outliers")) %>%
lapply(read_csv) %>%
bind_rows
#compile all the csv files to make one dataframe of all data
df <- list.files(path=setwd("~/Documents/GitHub/CSIA_lab_work/data/with_outliers")) %>%
lapply(read_csv) %>%
bind_rows
#Remove 5AA and REF columns
df <- df[!df$Sample.ID == "5AA",]
df <- df[,1:12]
#####Add Year column ####
year.2digit <- substr(df$Sample.ID, 1, 2)
year <- vector(mode="character")
for(i in 1:length(year.2digit)){
if(year.2digit[i] <= 22){
year[i] <- paste0(20, year.2digit[i])
} else{
year[i] <- paste0(19, year.2digit[i])
}
}
df$Year <- year
df <- df %>% relocate(Year, .before = VAL.mean)
#####Add System column####
sys <- substr(df$Sample.ID, 4, 4)
system <- vector(mode="character")
for(i in 1:length(sys)){
if(sys[i] == "W"){
system[i] <- "Wood"
} else if(sys[i] == "K"){
system[i] <- "Kvichak"
}  else{
system[i] <- "Egegik"
}
}
df$System <- system
df <- df %>% relocate(System, .before = VAL.mean)
#####Add Age column####
df$Age <- substr(df$Sample.ID, 6, 6)
df <- df %>% relocate(Age, .before = VAL.mean)
####Add column of replicates####
df$new.ID <- substr(df$Sample.ID, 1, 6) #new.ID gets rid of R in sample.ID
rep <- substr(df$Sample.ID, 8, 8)
df$rep <- substr(df$Sample.ID, 8, 8)
#function to average duplicate/replicates and replace in data file with new averages
#run this function as many times as replicates there are
rm_duplicates <- function(df, ID, Year, System, Age){
a <- subset(df, new.ID == ID)
b <- as.data.frame(a[,6:15])
vec <- vector(mode="numeric", length=10)
for(i in 1:10){
vec[i] <- mean(as.numeric(b[,i]))
}
c <- append(c(1, ID, Year, System, Age),c(vec, ID, 0))
norep <- df[!df$new.ID==ID,]
new.data <- rbind(norep, c)
print(new.data)
}
df <- rm_duplicates(df = df, ID = "01_E_3", Year = "2001", System = "Egegik", Age = "3")
df <- rm_duplicates(df = df, ID = "22_K_3", Year = "2022", System = "Kvichak", Age = "3")
df <- rm_duplicates(df = df, ID = "22_W_3", Year = "2022", System = "Wood", Age = "3")
df <- rm_duplicates(df = df, ID = "01_E_3", Year = "2001", System = "Egegik", Age = "3")
df <- rm_duplicates(df = df, ID = "22_K_3", Year = "2022", System = "Kvichak", Age = "3")
df <- rm_duplicates(df = df, ID = "22_W_3", Year = "2022", System = "Wood", Age = "3")
df <- rm_duplicates(df = df, ID = "13_W_2", Year = "2013", System = "Wood", Age = "2")
df <- rm_duplicates(df = df, ID = "13_W_3", Year = "2013", System = "Wood", Age = "3")
df <- rm_duplicates(df = df, ID = "04_E_2", Year = "2004", System = "Egegik", Age = "2")
df <- rm_duplicates(df = df, ID = "10_W_2", Year = "2010", System = "Wood", Age = "2")
df <- rm_duplicates(df = df, ID = "22_E_2", Year = "2022", System = "Egegik", Age = "2")
df <- rm_duplicates(df = df, ID = "74_W_2", Year = "1974", System = "Wood", Age = "2")
df <- rm_duplicates(df = df, ID = "89_K_2", Year = "1989", System = "Kvichak", Age = "2")
df <- rm_duplicates(df = df, ID = "89_W_2", Year = "1989", System = "Wood", Age = "2")
df <- rm_duplicates(df = df, ID = "74_E_2", Year = "1974", System = "Egegik", Age = "2")
#check to see if there are any duplicate samples left
anyDuplicated(df$new.ID)
#remove the last two columns
data <- df
data <- as.data.frame(data[,1:15])
#define beta and TDF values, this can be changed later if necessary
beta <- 3.4 #commonly used constant
TDF <- 7.06 #from Lerner et al 2020
#make an empty data frame to fill with Sample.ID and trophic position
tp <- data.frame(matrix(nrow = length(data$Sample.ID), ncol = 2))
tp <-setNames(tp, c("Sample.ID","Trophic.Position"))
#for loop to calculate trophic position and fill data frame
for(i in 1:length(data$Sample.ID)){
tp[i,2] <- 1 + ((as.numeric(data$GLU.mean[i])-as.numeric(data$PHE.mean[i])-beta)/TDF)
tp[i,1] <- data$Sample.ID[i]
}
#combine new data frame with original
data <- cbind(data, tp)
data <- data[, 3:17]
View(data)
#write new file
file.name <- "~/Documents/GitHub/CSIA_lab_work/data/final/data.csv"
write.csv(data, file.name)
library(openxlsx)
library(openxlsx)
write.xlsx(data, "~/Documents/GitHub/CSIA_lab_work/data/final/data.xlsx")
setwd("~/Documents/GitHub/CSIA_lab_work/data/final")
data <- read.csv(file = "data.csv")
library(ggplot2)
ggplot(data, aes(x = Year,
y = PHE.mean)) +
geom_point(size = 3, alpha = 0.7)
#load packages
install.packages(changepoint)
#load packages
install.packages(changepoint.np)
install.packages("changepoint")
#load packages
library(changepoint)
ggplot(data, aes(x = Year,
y = PHE.mean)) +
geom_point(size = 3, alpha = 0.7)
data$Year[1]
# Convert data to a time series object
ts_data <- ts(data$PHE.mean, start = data$Year[1])
# Convert data to a time series object
ts_data <- ts(data$PHE.mean)
# Reverse the order of rows in the data frame so that the oldest year is at the top
data <- data[order(data$Year, decreasing = TRUE), ]
data
# Reverse the order of rows in the data frame so that the oldest year is at the top
data <- data[order(data$Year, decreasing = TRUE), ]
# Convert data to a time series object
ts_data <- ts(data$PHE.mean, start = data$Year[length(data$Year)], end = data$Year[1])
# Perform change point analysis using the 'cpt.mean' function
cpt_result <- cpt.mean(ts_data)
# Remove rows with NAs
data <- na.omit(data)
# Reverse the order of rows in the data frame so that the oldest year is at the top
data <- data[order(data$Year, decreasing = TRUE), ]
# Convert data to a time series object
ts_data <- ts(data$PHE.mean, start = data$Year[length(data$Year)], end = data$Year[1])
# Perform change point analysis using the 'cpt.mean' function
cpt_result <- cpt.mean(ts_data)
# Plot the change point analysis results
plot(cpt_result, cpt.col = "blue")
cpt_result
summary(cpt_result)
View(cpt_result)
cpt_result.2 <- cpt.mean(ts_data, penalty="Manual", pen.value=2)
# Plot the change point analysis results
plot(cpt_result.2, cpt.col = "blue")
cpt_result.2 <- cpt.mean(ts_data, penalty="Manual", pen.value=2)
# Plot the change point analysis results
plot(cpt_result.2, cpt.col = "blue")
View(cpt_result.2)
ts_data
View(data)
# Convert the "Year" column to a factor to ensure correct ordering
data$Year <- factor(data$Year, levels = rev(unique(data$Year)))
data$Year
# Pivot the data to wide format to create separate columns for each river and age combination
library(tidyr)
# Pivot the data to wide format to create separate columns for each river and age combination
library(tidyr)
data_wide <- spread(data, System, PHE.mean)
View(data_wide)
data_wide <- pivot_wider(data, names_from = System, values_from = PHE.mean)
View(data_wide)
rm(data_wide)
data_wide <- pivot_wider(data, names_from = System, values_from = PHE.mean)
View(data_wide)
data_wide$Egegik[data_wide$Age == 2]
setwd("~/Documents/GitHub/CSIA_lab_work/data/final")
function (..., list = character(), package = NULL, lib.loc = NULL,
verbose = getOption("verbose"), envir = .GlobalEnv, overwrite = TRUE)
